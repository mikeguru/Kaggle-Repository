{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# Hello World\n",
    "# import library\n",
    "import tensorflow as tf\n",
    "# Create a Constant op\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "# Start tf session\n",
    "sess = tf.Session()\n",
    "# Run graph\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 2 b: 3\n",
      "Addition with constants: 5\n",
      "Multiplication with constants: 6\n",
      "Addition with variables: 5\n",
      "Multiplication with variables: 6\n",
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "# Basic Operations\n",
    "\n",
    "a = tf.constant(2)\n",
    "b = tf.constant(3)\n",
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    print(\"a: %i\" % sess.run(a), \"b: %i\" % sess.run(b))\n",
    "    print(\"Addition with constants: %i\" % sess.run(a+b))\n",
    "    print(\"Multiplication with constants: %i\" % sess.run(a*b))\n",
    "\n",
    "# input by the user\n",
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)\n",
    "\n",
    "# Define some operations\n",
    "add = tf.add(a, b)\n",
    "mul = tf.multiply(a, b)\n",
    "\n",
    "# Launch the default graph.\n",
    "with tf.Session() as sess:\n",
    "    # Run every operation with variable input\n",
    "    print(\"Addition with variables: %i\" % sess.run(add, feed_dict={a: 2, b: 3}))\n",
    "    print(\"Multiplication with variables: %i\" % sess.run(mul, feed_dict={a: 2, b: 3}))\n",
    "    \n",
    "# Matrix Multiplication\n",
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.],[2.]])\n",
    "product = tf.matmul(matrix1, matrix2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Test 0 Prediction: 7 True Class: 7\n",
      "Test 1 Prediction: 2 True Class: 2\n",
      "Test 2 Prediction: 1 True Class: 1\n",
      "Test 3 Prediction: 0 True Class: 0\n",
      "Test 4 Prediction: 4 True Class: 4\n",
      "Test 5 Prediction: 1 True Class: 1\n",
      "Test 6 Prediction: 4 True Class: 4\n",
      "Test 7 Prediction: 9 True Class: 9\n",
      "Test 8 Prediction: 8 True Class: 5\n",
      "Test 9 Prediction: 9 True Class: 9\n",
      "Test 10 Prediction: 0 True Class: 0\n",
      "Test 11 Prediction: 0 True Class: 6\n",
      "Test 12 Prediction: 9 True Class: 9\n",
      "Test 13 Prediction: 0 True Class: 0\n",
      "Test 14 Prediction: 1 True Class: 1\n",
      "Test 15 Prediction: 5 True Class: 5\n",
      "Test 16 Prediction: 4 True Class: 9\n",
      "Test 17 Prediction: 7 True Class: 7\n",
      "Test 18 Prediction: 3 True Class: 3\n",
      "Test 19 Prediction: 4 True Class: 4\n",
      "Test 20 Prediction: 9 True Class: 9\n",
      "Test 21 Prediction: 6 True Class: 6\n",
      "Test 22 Prediction: 6 True Class: 6\n",
      "Test 23 Prediction: 5 True Class: 5\n",
      "Test 24 Prediction: 4 True Class: 4\n",
      "Test 25 Prediction: 0 True Class: 0\n",
      "Test 26 Prediction: 7 True Class: 7\n",
      "Test 27 Prediction: 4 True Class: 4\n",
      "Test 28 Prediction: 0 True Class: 0\n",
      "Test 29 Prediction: 1 True Class: 1\n",
      "Test 30 Prediction: 3 True Class: 3\n",
      "Test 31 Prediction: 1 True Class: 1\n",
      "Test 32 Prediction: 3 True Class: 3\n",
      "Test 33 Prediction: 4 True Class: 4\n",
      "Test 34 Prediction: 7 True Class: 7\n",
      "Test 35 Prediction: 2 True Class: 2\n",
      "Test 36 Prediction: 7 True Class: 7\n",
      "Test 37 Prediction: 1 True Class: 1\n",
      "Test 38 Prediction: 2 True Class: 2\n",
      "Test 39 Prediction: 1 True Class: 1\n",
      "Test 40 Prediction: 1 True Class: 1\n",
      "Test 41 Prediction: 7 True Class: 7\n",
      "Test 42 Prediction: 4 True Class: 4\n",
      "Test 43 Prediction: 1 True Class: 2\n",
      "Test 44 Prediction: 3 True Class: 3\n",
      "Test 45 Prediction: 5 True Class: 5\n",
      "Test 46 Prediction: 1 True Class: 1\n",
      "Test 47 Prediction: 2 True Class: 2\n",
      "Test 48 Prediction: 4 True Class: 4\n",
      "Test 49 Prediction: 4 True Class: 4\n",
      "Test 50 Prediction: 6 True Class: 6\n",
      "Test 51 Prediction: 3 True Class: 3\n",
      "Test 52 Prediction: 5 True Class: 5\n",
      "Test 53 Prediction: 5 True Class: 5\n",
      "Test 54 Prediction: 6 True Class: 6\n",
      "Test 55 Prediction: 0 True Class: 0\n",
      "Test 56 Prediction: 4 True Class: 4\n",
      "Test 57 Prediction: 1 True Class: 1\n",
      "Test 58 Prediction: 9 True Class: 9\n",
      "Test 59 Prediction: 5 True Class: 5\n",
      "Test 60 Prediction: 7 True Class: 7\n",
      "Test 61 Prediction: 8 True Class: 8\n",
      "Test 62 Prediction: 9 True Class: 9\n",
      "Test 63 Prediction: 3 True Class: 3\n",
      "Test 64 Prediction: 7 True Class: 7\n",
      "Test 65 Prediction: 4 True Class: 4\n",
      "Test 66 Prediction: 6 True Class: 6\n",
      "Test 67 Prediction: 4 True Class: 4\n",
      "Test 68 Prediction: 3 True Class: 3\n",
      "Test 69 Prediction: 0 True Class: 0\n",
      "Test 70 Prediction: 7 True Class: 7\n",
      "Test 71 Prediction: 0 True Class: 0\n",
      "Test 72 Prediction: 2 True Class: 2\n",
      "Test 73 Prediction: 7 True Class: 9\n",
      "Test 74 Prediction: 1 True Class: 1\n",
      "Test 75 Prediction: 7 True Class: 7\n",
      "Test 76 Prediction: 3 True Class: 3\n",
      "Test 77 Prediction: 7 True Class: 2\n",
      "Test 78 Prediction: 9 True Class: 9\n",
      "Test 79 Prediction: 7 True Class: 7\n",
      "Test 80 Prediction: 7 True Class: 7\n",
      "Test 81 Prediction: 6 True Class: 6\n",
      "Test 82 Prediction: 2 True Class: 2\n",
      "Test 83 Prediction: 7 True Class: 7\n",
      "Test 84 Prediction: 8 True Class: 8\n",
      "Test 85 Prediction: 4 True Class: 4\n",
      "Test 86 Prediction: 7 True Class: 7\n",
      "Test 87 Prediction: 3 True Class: 3\n",
      "Test 88 Prediction: 6 True Class: 6\n",
      "Test 89 Prediction: 1 True Class: 1\n",
      "Test 90 Prediction: 3 True Class: 3\n",
      "Test 91 Prediction: 6 True Class: 6\n",
      "Test 92 Prediction: 9 True Class: 9\n",
      "Test 93 Prediction: 3 True Class: 3\n",
      "Test 94 Prediction: 1 True Class: 1\n",
      "Test 95 Prediction: 4 True Class: 4\n",
      "Test 96 Prediction: 1 True Class: 1\n",
      "Test 97 Prediction: 7 True Class: 7\n",
      "Test 98 Prediction: 6 True Class: 6\n",
      "Test 99 Prediction: 9 True Class: 9\n",
      "Test 100 Prediction: 6 True Class: 6\n",
      "Test 101 Prediction: 0 True Class: 0\n",
      "Test 102 Prediction: 5 True Class: 5\n",
      "Test 103 Prediction: 4 True Class: 4\n",
      "Test 104 Prediction: 9 True Class: 9\n",
      "Test 105 Prediction: 9 True Class: 9\n",
      "Test 106 Prediction: 2 True Class: 2\n",
      "Test 107 Prediction: 1 True Class: 1\n",
      "Test 108 Prediction: 9 True Class: 9\n",
      "Test 109 Prediction: 4 True Class: 4\n",
      "Test 110 Prediction: 8 True Class: 8\n",
      "Test 111 Prediction: 7 True Class: 7\n",
      "Test 112 Prediction: 3 True Class: 3\n",
      "Test 113 Prediction: 9 True Class: 9\n",
      "Test 114 Prediction: 7 True Class: 7\n",
      "Test 115 Prediction: 9 True Class: 4\n",
      "Test 116 Prediction: 9 True Class: 4\n",
      "Test 117 Prediction: 4 True Class: 4\n",
      "Test 118 Prediction: 9 True Class: 9\n",
      "Test 119 Prediction: 7 True Class: 2\n",
      "Test 120 Prediction: 5 True Class: 5\n",
      "Test 121 Prediction: 4 True Class: 4\n",
      "Test 122 Prediction: 7 True Class: 7\n",
      "Test 123 Prediction: 6 True Class: 6\n",
      "Test 124 Prediction: 7 True Class: 7\n",
      "Test 125 Prediction: 9 True Class: 9\n",
      "Test 126 Prediction: 0 True Class: 0\n",
      "Test 127 Prediction: 5 True Class: 5\n",
      "Test 128 Prediction: 8 True Class: 8\n",
      "Test 129 Prediction: 5 True Class: 5\n",
      "Test 130 Prediction: 6 True Class: 6\n",
      "Test 131 Prediction: 6 True Class: 6\n",
      "Test 132 Prediction: 5 True Class: 5\n",
      "Test 133 Prediction: 7 True Class: 7\n",
      "Test 134 Prediction: 8 True Class: 8\n",
      "Test 135 Prediction: 1 True Class: 1\n",
      "Test 136 Prediction: 0 True Class: 0\n",
      "Test 137 Prediction: 1 True Class: 1\n",
      "Test 138 Prediction: 6 True Class: 6\n",
      "Test 139 Prediction: 4 True Class: 4\n",
      "Test 140 Prediction: 6 True Class: 6\n",
      "Test 141 Prediction: 7 True Class: 7\n",
      "Test 142 Prediction: 2 True Class: 3\n",
      "Test 143 Prediction: 1 True Class: 1\n",
      "Test 144 Prediction: 7 True Class: 7\n",
      "Test 145 Prediction: 1 True Class: 1\n",
      "Test 146 Prediction: 8 True Class: 8\n",
      "Test 147 Prediction: 2 True Class: 2\n",
      "Test 148 Prediction: 0 True Class: 0\n",
      "Test 149 Prediction: 1 True Class: 2\n",
      "Test 150 Prediction: 9 True Class: 9\n",
      "Test 151 Prediction: 9 True Class: 9\n",
      "Test 152 Prediction: 5 True Class: 5\n",
      "Test 153 Prediction: 5 True Class: 5\n",
      "Test 154 Prediction: 1 True Class: 1\n",
      "Test 155 Prediction: 5 True Class: 5\n",
      "Test 156 Prediction: 6 True Class: 6\n",
      "Test 157 Prediction: 0 True Class: 0\n",
      "Test 158 Prediction: 3 True Class: 3\n",
      "Test 159 Prediction: 4 True Class: 4\n",
      "Test 160 Prediction: 4 True Class: 4\n",
      "Test 161 Prediction: 6 True Class: 6\n",
      "Test 162 Prediction: 5 True Class: 5\n",
      "Test 163 Prediction: 4 True Class: 4\n",
      "Test 164 Prediction: 6 True Class: 6\n",
      "Test 165 Prediction: 5 True Class: 5\n",
      "Test 166 Prediction: 4 True Class: 4\n",
      "Test 167 Prediction: 5 True Class: 5\n",
      "Test 168 Prediction: 1 True Class: 1\n",
      "Test 169 Prediction: 4 True Class: 4\n",
      "Test 170 Prediction: 9 True Class: 4\n",
      "Test 171 Prediction: 7 True Class: 7\n",
      "Test 172 Prediction: 2 True Class: 2\n",
      "Test 173 Prediction: 3 True Class: 3\n",
      "Test 174 Prediction: 2 True Class: 2\n",
      "Test 175 Prediction: 1 True Class: 7\n",
      "Test 176 Prediction: 1 True Class: 1\n",
      "Test 177 Prediction: 8 True Class: 8\n",
      "Test 178 Prediction: 1 True Class: 1\n",
      "Test 179 Prediction: 8 True Class: 8\n",
      "Test 180 Prediction: 1 True Class: 1\n",
      "Test 181 Prediction: 8 True Class: 8\n",
      "Test 182 Prediction: 5 True Class: 5\n",
      "Test 183 Prediction: 0 True Class: 0\n",
      "Test 184 Prediction: 2 True Class: 8\n",
      "Test 185 Prediction: 9 True Class: 9\n",
      "Test 186 Prediction: 2 True Class: 2\n",
      "Test 187 Prediction: 5 True Class: 5\n",
      "Test 188 Prediction: 0 True Class: 0\n",
      "Test 189 Prediction: 1 True Class: 1\n",
      "Test 190 Prediction: 1 True Class: 1\n",
      "Test 191 Prediction: 1 True Class: 1\n",
      "Test 192 Prediction: 0 True Class: 0\n",
      "Test 193 Prediction: 4 True Class: 9\n",
      "Test 194 Prediction: 0 True Class: 0\n",
      "Test 195 Prediction: 1 True Class: 3\n",
      "Test 196 Prediction: 1 True Class: 1\n",
      "Test 197 Prediction: 6 True Class: 6\n",
      "Test 198 Prediction: 4 True Class: 4\n",
      "Test 199 Prediction: 2 True Class: 2\n",
      "Done!\n",
      "Accuracy: 0.9200000000000007\n"
     ]
    }
   ],
   "source": [
    "# knn model\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# In this example, we limit mnist data\n",
    "Xtr, Ytr = mnist.train.next_batch(5000) #5000 for training (nn candidates)\n",
    "Xte, Yte = mnist.test.next_batch(200) #200 for testing\n",
    "\n",
    "# tf Graph Input\n",
    "xtr = tf.placeholder(\"float\", [None, 784])\n",
    "xte = tf.placeholder(\"float\", [784])\n",
    "\n",
    "# Nearest Neighbor calculation using L1 Distance\n",
    "# Calculate L1 Distance\n",
    "distance = tf.reduce_sum(tf.abs(tf.add(xtr, tf.negative(xte))), reduction_indices=1)\n",
    "# Prediction: Get min distance index (Nearest neighbor)\n",
    "pred = tf.arg_min(distance, 0)\n",
    "\n",
    "accuracy = 0.\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # loop over test data\n",
    "    for i in range(len(Xte)):\n",
    "        # Get nearest neighbor\n",
    "        nn_index = sess.run(pred, feed_dict={xtr: Xtr, xte: Xte[i, :]})\n",
    "        # Get nearest neighbor class label and compare it to its true label\n",
    "        print(\"Test\", i, \"Prediction:\", np.argmax(Ytr[nn_index]), \\\n",
    "            \"True Class:\", np.argmax(Yte[i]))\n",
    "        # Calculate accuracy\n",
    "        if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n",
    "            accuracy += 1./len(Xte)\n",
    "    print(\"Done!\")\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost= 0.084687769 W= 0.298822 b= 0.447323\n",
      "Epoch: 0100 cost= 0.083795324 W= 0.295894 b= 0.468386\n",
      "Epoch: 0150 cost= 0.083006114 W= 0.29314 b= 0.488196\n",
      "Epoch: 0200 cost= 0.082308233 W= 0.29055 b= 0.506827\n",
      "Epoch: 0250 cost= 0.081691101 W= 0.288114 b= 0.524351\n",
      "Epoch: 0300 cost= 0.081145421 W= 0.285823 b= 0.540832\n",
      "Epoch: 0350 cost= 0.080662884 W= 0.283668 b= 0.556334\n",
      "Epoch: 0400 cost= 0.080236189 W= 0.281642 b= 0.570914\n",
      "Epoch: 0450 cost= 0.079858959 W= 0.279736 b= 0.584627\n",
      "Epoch: 0500 cost= 0.079525404 W= 0.277943 b= 0.597524\n",
      "Epoch: 0550 cost= 0.079230458 W= 0.276256 b= 0.609656\n",
      "Epoch: 0600 cost= 0.078969762 W= 0.274671 b= 0.621063\n",
      "Epoch: 0650 cost= 0.078739263 W= 0.273179 b= 0.631792\n",
      "Epoch: 0700 cost= 0.078535505 W= 0.271777 b= 0.641882\n",
      "Epoch: 0750 cost= 0.078355379 W= 0.270457 b= 0.651373\n",
      "Epoch: 0800 cost= 0.078196160 W= 0.269216 b= 0.660301\n",
      "Epoch: 0850 cost= 0.078055397 W= 0.26805 b= 0.668696\n",
      "Epoch: 0900 cost= 0.077931009 W= 0.266952 b= 0.676592\n",
      "Epoch: 0950 cost= 0.077821031 W= 0.26592 b= 0.684019\n",
      "Epoch: 1000 cost= 0.077723853 W= 0.264949 b= 0.691004\n",
      "Optimization Finished!\n",
      "Training cost= 0.0777239 W= 0.264949 b= 0.691004 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24edfec5a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# linear regression model\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "rng = numpy.random\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 1000\n",
    "display_step = 50\n",
    "\n",
    "# Training Data\n",
    "train_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n",
    "                         7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n",
    "                         2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]\n",
    "\n",
    "# tf Graph Input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(rng.randn(), name=\"weight\")\n",
    "b = tf.Variable(rng.randn(), name=\"bias\")\n",
    "\n",
    "# Construct a linear model\n",
    "pred = tf.add(tf.multiply(X, W), b)\n",
    "\n",
    "# Mean squared error\n",
    "cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n",
    "# Gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit all training data\n",
    "    for epoch in range(training_epochs):\n",
    "        for (x, y) in zip(train_X, train_Y):\n",
    "            sess.run(optimizer, feed_dict={X: x, Y: y})\n",
    "\n",
    "        #Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={X: train_X, Y:train_Y})\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n",
    "                \"W=\", sess.run(W), \"b=\", sess.run(b))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n",
    "    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n",
    "\n",
    "    #Graphic display\n",
    "    plt.plot(train_X, train_Y, 'ro', label='Original data')\n",
    "    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.183631610\n",
      "Epoch: 0002 cost= 0.664262590\n",
      "Epoch: 0003 cost= 0.552666119\n",
      "Epoch: 0004 cost= 0.498312637\n",
      "Epoch: 0005 cost= 0.465612526\n",
      "Epoch: 0006 cost= 0.443138899\n",
      "Epoch: 0007 cost= 0.425522115\n",
      "Epoch: 0008 cost= 0.412097836\n",
      "Epoch: 0009 cost= 0.401667477\n",
      "Epoch: 0010 cost= 0.393848136\n",
      "Epoch: 0011 cost= 0.382010629\n",
      "Epoch: 0012 cost= 0.378736995\n",
      "Epoch: 0013 cost= 0.373492159\n",
      "Epoch: 0014 cost= 0.367074980\n",
      "Epoch: 0015 cost= 0.361369377\n",
      "Epoch: 0016 cost= 0.361139760\n",
      "Epoch: 0017 cost= 0.353623811\n",
      "Epoch: 0018 cost= 0.350017284\n",
      "Epoch: 0019 cost= 0.349614258\n",
      "Epoch: 0020 cost= 0.346005117\n",
      "Epoch: 0021 cost= 0.342044737\n",
      "Epoch: 0022 cost= 0.338777304\n",
      "Epoch: 0023 cost= 0.338330821\n",
      "Epoch: 0024 cost= 0.336146063\n",
      "Epoch: 0025 cost= 0.331902856\n",
      "Optimization Finished!\n",
      "Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Construct model\n",
    "pred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n",
    "# Gradient Descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            # Fit training using batch data\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n",
    "                                                          y: batch_ys})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 22949.685547, Training Accuracy= 0.25000\n",
      "Iter 2560, Minibatch Loss= 8314.550781, Training Accuracy= 0.53906\n",
      "Iter 3840, Minibatch Loss= 6899.398926, Training Accuracy= 0.67188\n",
      "Iter 5120, Minibatch Loss= 3678.613525, Training Accuracy= 0.78906\n",
      "Iter 6400, Minibatch Loss= 2737.688721, Training Accuracy= 0.80469\n",
      "Iter 7680, Minibatch Loss= 2225.326904, Training Accuracy= 0.86719\n",
      "Iter 8960, Minibatch Loss= 2901.610107, Training Accuracy= 0.80469\n",
      "Iter 10240, Minibatch Loss= 2414.794189, Training Accuracy= 0.87500\n",
      "Iter 11520, Minibatch Loss= 2058.649658, Training Accuracy= 0.88281\n",
      "Iter 12800, Minibatch Loss= 1320.031982, Training Accuracy= 0.88281\n",
      "Iter 14080, Minibatch Loss= 1671.514771, Training Accuracy= 0.89062\n",
      "Iter 15360, Minibatch Loss= 1750.470093, Training Accuracy= 0.86719\n",
      "Iter 16640, Minibatch Loss= 1743.327637, Training Accuracy= 0.86719\n",
      "Iter 17920, Minibatch Loss= 1253.091797, Training Accuracy= 0.92188\n",
      "Iter 19200, Minibatch Loss= 1685.619385, Training Accuracy= 0.89844\n",
      "Iter 20480, Minibatch Loss= 1561.425537, Training Accuracy= 0.92969\n",
      "Iter 21760, Minibatch Loss= 1673.340576, Training Accuracy= 0.87500\n",
      "Iter 23040, Minibatch Loss= 589.443298, Training Accuracy= 0.91406\n",
      "Iter 24320, Minibatch Loss= 1035.206421, Training Accuracy= 0.89062\n",
      "Iter 25600, Minibatch Loss= 2238.586914, Training Accuracy= 0.89844\n",
      "Iter 26880, Minibatch Loss= 1597.181885, Training Accuracy= 0.87500\n",
      "Iter 28160, Minibatch Loss= 263.713318, Training Accuracy= 0.97656\n",
      "Iter 29440, Minibatch Loss= 1300.262085, Training Accuracy= 0.92969\n",
      "Iter 30720, Minibatch Loss= 1683.550293, Training Accuracy= 0.91406\n",
      "Iter 32000, Minibatch Loss= 679.674011, Training Accuracy= 0.92188\n",
      "Iter 33280, Minibatch Loss= 430.657532, Training Accuracy= 0.95312\n",
      "Iter 34560, Minibatch Loss= 1418.698486, Training Accuracy= 0.89844\n",
      "Iter 35840, Minibatch Loss= 269.048462, Training Accuracy= 0.96094\n",
      "Iter 37120, Minibatch Loss= 1238.096436, Training Accuracy= 0.90625\n",
      "Iter 38400, Minibatch Loss= 621.898315, Training Accuracy= 0.95312\n",
      "Iter 39680, Minibatch Loss= 1287.935181, Training Accuracy= 0.92969\n",
      "Iter 40960, Minibatch Loss= 810.839722, Training Accuracy= 0.92188\n",
      "Iter 42240, Minibatch Loss= 929.010742, Training Accuracy= 0.93750\n",
      "Iter 43520, Minibatch Loss= 659.068359, Training Accuracy= 0.92188\n",
      "Iter 44800, Minibatch Loss= 805.150208, Training Accuracy= 0.92969\n",
      "Iter 46080, Minibatch Loss= 624.890137, Training Accuracy= 0.94531\n",
      "Iter 47360, Minibatch Loss= 310.777222, Training Accuracy= 0.95312\n",
      "Iter 48640, Minibatch Loss= 842.605774, Training Accuracy= 0.94531\n",
      "Iter 49920, Minibatch Loss= 899.207764, Training Accuracy= 0.91406\n",
      "Iter 51200, Minibatch Loss= 416.831482, Training Accuracy= 0.94531\n",
      "Iter 52480, Minibatch Loss= 857.271729, Training Accuracy= 0.93750\n",
      "Iter 53760, Minibatch Loss= 637.095459, Training Accuracy= 0.95312\n",
      "Iter 55040, Minibatch Loss= 841.330688, Training Accuracy= 0.92188\n",
      "Iter 56320, Minibatch Loss= 487.674072, Training Accuracy= 0.95312\n",
      "Iter 57600, Minibatch Loss= 460.324890, Training Accuracy= 0.93750\n",
      "Iter 58880, Minibatch Loss= 6.309784, Training Accuracy= 0.99219\n",
      "Iter 60160, Minibatch Loss= 714.068848, Training Accuracy= 0.92969\n",
      "Iter 61440, Minibatch Loss= 410.258545, Training Accuracy= 0.95312\n",
      "Iter 62720, Minibatch Loss= 567.294739, Training Accuracy= 0.95312\n",
      "Iter 64000, Minibatch Loss= 124.724899, Training Accuracy= 0.96875\n",
      "Iter 65280, Minibatch Loss= 629.042542, Training Accuracy= 0.96094\n",
      "Iter 66560, Minibatch Loss= 646.904785, Training Accuracy= 0.94531\n",
      "Iter 67840, Minibatch Loss= 845.925781, Training Accuracy= 0.92969\n",
      "Iter 69120, Minibatch Loss= 582.524780, Training Accuracy= 0.97656\n",
      "Iter 70400, Minibatch Loss= 369.117432, Training Accuracy= 0.96875\n",
      "Iter 71680, Minibatch Loss= 356.598999, Training Accuracy= 0.94531\n",
      "Iter 72960, Minibatch Loss= 467.614563, Training Accuracy= 0.96094\n",
      "Iter 74240, Minibatch Loss= 302.275299, Training Accuracy= 0.94531\n",
      "Iter 75520, Minibatch Loss= 580.797363, Training Accuracy= 0.91406\n",
      "Iter 76800, Minibatch Loss= 437.039368, Training Accuracy= 0.95312\n",
      "Iter 78080, Minibatch Loss= 300.623871, Training Accuracy= 0.96875\n",
      "Iter 79360, Minibatch Loss= 262.672516, Training Accuracy= 0.95312\n",
      "Iter 80640, Minibatch Loss= 328.271423, Training Accuracy= 0.93750\n",
      "Iter 81920, Minibatch Loss= 452.203125, Training Accuracy= 0.95312\n",
      "Iter 83200, Minibatch Loss= 638.943359, Training Accuracy= 0.94531\n",
      "Iter 84480, Minibatch Loss= 489.870026, Training Accuracy= 0.95312\n",
      "Iter 85760, Minibatch Loss= 295.916382, Training Accuracy= 0.96094\n",
      "Iter 87040, Minibatch Loss= 273.245270, Training Accuracy= 0.98438\n",
      "Iter 88320, Minibatch Loss= 420.606354, Training Accuracy= 0.93750\n",
      "Iter 89600, Minibatch Loss= 488.452148, Training Accuracy= 0.96094\n",
      "Iter 90880, Minibatch Loss= 590.847778, Training Accuracy= 0.93750\n",
      "Iter 92160, Minibatch Loss= 271.235016, Training Accuracy= 0.95312\n",
      "Iter 93440, Minibatch Loss= 372.682068, Training Accuracy= 0.94531\n",
      "Iter 94720, Minibatch Loss= 408.164368, Training Accuracy= 0.93750\n",
      "Iter 96000, Minibatch Loss= 239.335297, Training Accuracy= 0.97656\n",
      "Iter 97280, Minibatch Loss= 164.660324, Training Accuracy= 0.96094\n",
      "Iter 98560, Minibatch Loss= 694.168823, Training Accuracy= 0.94531\n",
      "Iter 99840, Minibatch Loss= 498.951965, Training Accuracy= 0.97656\n",
      "Iter 101120, Minibatch Loss= 325.381317, Training Accuracy= 0.97656\n",
      "Iter 102400, Minibatch Loss= 387.901001, Training Accuracy= 0.95312\n",
      "Iter 103680, Minibatch Loss= 289.072205, Training Accuracy= 0.96875\n",
      "Iter 104960, Minibatch Loss= 385.937561, Training Accuracy= 0.97656\n",
      "Iter 106240, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 107520, Minibatch Loss= 150.125610, Training Accuracy= 0.97656\n",
      "Iter 108800, Minibatch Loss= 378.026062, Training Accuracy= 0.96094\n",
      "Iter 110080, Minibatch Loss= 100.085114, Training Accuracy= 0.99219\n",
      "Iter 111360, Minibatch Loss= 160.379730, Training Accuracy= 0.96875\n",
      "Iter 112640, Minibatch Loss= 377.627838, Training Accuracy= 0.96094\n",
      "Iter 113920, Minibatch Loss= 340.137451, Training Accuracy= 0.96094\n",
      "Iter 115200, Minibatch Loss= 98.358871, Training Accuracy= 0.96094\n",
      "Iter 116480, Minibatch Loss= 45.683746, Training Accuracy= 0.98438\n",
      "Iter 117760, Minibatch Loss= 462.464386, Training Accuracy= 0.96094\n",
      "Iter 119040, Minibatch Loss= 186.801270, Training Accuracy= 0.97656\n",
      "Iter 120320, Minibatch Loss= 438.145813, Training Accuracy= 0.95312\n",
      "Iter 121600, Minibatch Loss= 206.285172, Training Accuracy= 0.96094\n",
      "Iter 122880, Minibatch Loss= 116.411751, Training Accuracy= 0.98438\n",
      "Iter 124160, Minibatch Loss= 176.252441, Training Accuracy= 0.97656\n",
      "Iter 125440, Minibatch Loss= 245.150757, Training Accuracy= 0.95312\n",
      "Iter 126720, Minibatch Loss= 316.231873, Training Accuracy= 0.95312\n",
      "Iter 128000, Minibatch Loss= 51.421364, Training Accuracy= 0.98438\n",
      "Iter 129280, Minibatch Loss= 121.837387, Training Accuracy= 0.98438\n",
      "Iter 130560, Minibatch Loss= 171.394638, Training Accuracy= 0.94531\n",
      "Iter 131840, Minibatch Loss= 220.564545, Training Accuracy= 0.96875\n",
      "Iter 133120, Minibatch Loss= 234.343597, Training Accuracy= 0.96875\n",
      "Iter 134400, Minibatch Loss= 28.823067, Training Accuracy= 0.97656\n",
      "Iter 135680, Minibatch Loss= 130.447540, Training Accuracy= 0.98438\n",
      "Iter 136960, Minibatch Loss= 562.646484, Training Accuracy= 0.96094\n",
      "Iter 138240, Minibatch Loss= 0.244530, Training Accuracy= 0.99219\n",
      "Iter 139520, Minibatch Loss= 199.092209, Training Accuracy= 0.96094\n",
      "Iter 140800, Minibatch Loss= 118.690399, Training Accuracy= 0.99219\n",
      "Iter 142080, Minibatch Loss= 342.806976, Training Accuracy= 0.96875\n",
      "Iter 143360, Minibatch Loss= 414.959351, Training Accuracy= 0.96094\n",
      "Iter 144640, Minibatch Loss= 235.633301, Training Accuracy= 0.98438\n",
      "Iter 145920, Minibatch Loss= 138.628693, Training Accuracy= 0.98438\n",
      "Iter 147200, Minibatch Loss= 343.674683, Training Accuracy= 0.92969\n",
      "Iter 148480, Minibatch Loss= 162.748581, Training Accuracy= 0.97656\n",
      "Iter 149760, Minibatch Loss= 533.064148, Training Accuracy= 0.94531\n",
      "Iter 151040, Minibatch Loss= 52.861195, Training Accuracy= 0.96875\n",
      "Iter 152320, Minibatch Loss= 256.910034, Training Accuracy= 0.95312\n",
      "Iter 153600, Minibatch Loss= 179.959183, Training Accuracy= 0.96875\n",
      "Iter 154880, Minibatch Loss= 92.212807, Training Accuracy= 0.98438\n",
      "Iter 156160, Minibatch Loss= 150.034363, Training Accuracy= 0.96094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 157440, Minibatch Loss= 152.197647, Training Accuracy= 0.98438\n",
      "Iter 158720, Minibatch Loss= 217.270432, Training Accuracy= 0.97656\n",
      "Iter 160000, Minibatch Loss= 137.869995, Training Accuracy= 0.99219\n",
      "Iter 161280, Minibatch Loss= 178.143402, Training Accuracy= 0.97656\n",
      "Iter 162560, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 163840, Minibatch Loss= 214.566513, Training Accuracy= 0.94531\n",
      "Iter 165120, Minibatch Loss= 61.067631, Training Accuracy= 0.98438\n",
      "Iter 166400, Minibatch Loss= 187.943207, Training Accuracy= 0.97656\n",
      "Iter 167680, Minibatch Loss= 256.870850, Training Accuracy= 0.96875\n",
      "Iter 168960, Minibatch Loss= 46.615784, Training Accuracy= 0.99219\n",
      "Iter 170240, Minibatch Loss= 0.326141, Training Accuracy= 0.99219\n",
      "Iter 171520, Minibatch Loss= 44.093201, Training Accuracy= 0.97656\n",
      "Iter 172800, Minibatch Loss= 129.367828, Training Accuracy= 0.97656\n",
      "Iter 174080, Minibatch Loss= 377.065186, Training Accuracy= 0.94531\n",
      "Iter 175360, Minibatch Loss= 170.832031, Training Accuracy= 0.96875\n",
      "Iter 176640, Minibatch Loss= 141.201065, Training Accuracy= 0.96875\n",
      "Iter 177920, Minibatch Loss= 155.218918, Training Accuracy= 0.97656\n",
      "Iter 179200, Minibatch Loss= 113.723389, Training Accuracy= 0.98438\n",
      "Iter 180480, Minibatch Loss= 242.078400, Training Accuracy= 0.94531\n",
      "Iter 181760, Minibatch Loss= 20.417587, Training Accuracy= 0.99219\n",
      "Iter 183040, Minibatch Loss= 32.456074, Training Accuracy= 0.99219\n",
      "Iter 184320, Minibatch Loss= 42.607948, Training Accuracy= 0.99219\n",
      "Iter 185600, Minibatch Loss= 0.394028, Training Accuracy= 0.99219\n",
      "Iter 186880, Minibatch Loss= 275.335876, Training Accuracy= 0.96875\n",
      "Iter 188160, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 189440, Minibatch Loss= 83.216797, Training Accuracy= 0.99219\n",
      "Iter 190720, Minibatch Loss= 193.262970, Training Accuracy= 0.97656\n",
      "Iter 192000, Minibatch Loss= 218.926544, Training Accuracy= 0.96094\n",
      "Iter 193280, Minibatch Loss= 80.063240, Training Accuracy= 0.97656\n",
      "Iter 194560, Minibatch Loss= 207.441849, Training Accuracy= 0.98438\n",
      "Iter 195840, Minibatch Loss= 311.017975, Training Accuracy= 0.96094\n",
      "Iter 197120, Minibatch Loss= 134.969269, Training Accuracy= 0.96094\n",
      "Iter 198400, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 199680, Minibatch Loss= 24.502777, Training Accuracy= 0.98438\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.980469\n"
     ]
    }
   ],
   "source": [
    "# convolutional network model\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 5x5 conv, 1 input, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "    # 5x5 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "    # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "    # 1024 inputs, 10 outputs (class prediction)\n",
    "    'out': tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 2.105906, Training Accuracy= 0.21094\n",
      "Iter 2560, Minibatch Loss= 1.799792, Training Accuracy= 0.37500\n",
      "Iter 3840, Minibatch Loss= 1.567945, Training Accuracy= 0.42969\n",
      "Iter 5120, Minibatch Loss= 1.227663, Training Accuracy= 0.58594\n",
      "Iter 6400, Minibatch Loss= 1.116256, Training Accuracy= 0.65625\n",
      "Iter 7680, Minibatch Loss= 0.873246, Training Accuracy= 0.70312\n",
      "Iter 8960, Minibatch Loss= 0.784604, Training Accuracy= 0.69531\n",
      "Iter 10240, Minibatch Loss= 0.643670, Training Accuracy= 0.77344\n",
      "Iter 11520, Minibatch Loss= 0.470302, Training Accuracy= 0.85156\n",
      "Iter 12800, Minibatch Loss= 0.395306, Training Accuracy= 0.87500\n",
      "Iter 14080, Minibatch Loss= 0.417268, Training Accuracy= 0.85938\n",
      "Iter 15360, Minibatch Loss= 0.407420, Training Accuracy= 0.86719\n",
      "Iter 16640, Minibatch Loss= 0.612570, Training Accuracy= 0.78125\n",
      "Iter 17920, Minibatch Loss= 0.311140, Training Accuracy= 0.89062\n",
      "Iter 19200, Minibatch Loss= 0.351058, Training Accuracy= 0.88281\n",
      "Iter 20480, Minibatch Loss= 0.240040, Training Accuracy= 0.92969\n",
      "Iter 21760, Minibatch Loss= 0.498771, Training Accuracy= 0.85156\n",
      "Iter 23040, Minibatch Loss= 0.312333, Training Accuracy= 0.89062\n",
      "Iter 24320, Minibatch Loss= 0.218952, Training Accuracy= 0.94531\n",
      "Iter 25600, Minibatch Loss= 0.265062, Training Accuracy= 0.90625\n",
      "Iter 26880, Minibatch Loss= 0.323242, Training Accuracy= 0.92969\n",
      "Iter 28160, Minibatch Loss= 0.235956, Training Accuracy= 0.92188\n",
      "Iter 29440, Minibatch Loss= 0.191006, Training Accuracy= 0.93750\n",
      "Iter 30720, Minibatch Loss= 0.119261, Training Accuracy= 0.96875\n",
      "Iter 32000, Minibatch Loss= 0.143353, Training Accuracy= 0.95312\n",
      "Iter 33280, Minibatch Loss= 0.158884, Training Accuracy= 0.94531\n",
      "Iter 34560, Minibatch Loss= 0.140475, Training Accuracy= 0.94531\n",
      "Iter 35840, Minibatch Loss= 0.145514, Training Accuracy= 0.94531\n",
      "Iter 37120, Minibatch Loss= 0.204575, Training Accuracy= 0.93750\n",
      "Iter 38400, Minibatch Loss= 0.196422, Training Accuracy= 0.95312\n",
      "Iter 39680, Minibatch Loss= 0.176326, Training Accuracy= 0.92969\n",
      "Iter 40960, Minibatch Loss= 0.205343, Training Accuracy= 0.94531\n",
      "Iter 42240, Minibatch Loss= 0.117778, Training Accuracy= 0.95312\n",
      "Iter 43520, Minibatch Loss= 0.138739, Training Accuracy= 0.96875\n",
      "Iter 44800, Minibatch Loss= 0.063140, Training Accuracy= 0.98438\n",
      "Iter 46080, Minibatch Loss= 0.211932, Training Accuracy= 0.92969\n",
      "Iter 47360, Minibatch Loss= 0.144465, Training Accuracy= 0.95312\n",
      "Iter 48640, Minibatch Loss= 0.143766, Training Accuracy= 0.93750\n",
      "Iter 49920, Minibatch Loss= 0.148462, Training Accuracy= 0.95312\n",
      "Iter 51200, Minibatch Loss= 0.106696, Training Accuracy= 0.96094\n",
      "Iter 52480, Minibatch Loss= 0.074320, Training Accuracy= 0.97656\n",
      "Iter 53760, Minibatch Loss= 0.199289, Training Accuracy= 0.96094\n",
      "Iter 55040, Minibatch Loss= 0.087912, Training Accuracy= 0.97656\n",
      "Iter 56320, Minibatch Loss= 0.145533, Training Accuracy= 0.95312\n",
      "Iter 57600, Minibatch Loss= 0.102875, Training Accuracy= 0.95312\n",
      "Iter 58880, Minibatch Loss= 0.117416, Training Accuracy= 0.96094\n",
      "Iter 60160, Minibatch Loss= 0.192198, Training Accuracy= 0.94531\n",
      "Iter 61440, Minibatch Loss= 0.113831, Training Accuracy= 0.96875\n",
      "Iter 62720, Minibatch Loss= 0.072181, Training Accuracy= 0.97656\n",
      "Iter 64000, Minibatch Loss= 0.116451, Training Accuracy= 0.96094\n",
      "Iter 65280, Minibatch Loss= 0.134775, Training Accuracy= 0.95312\n",
      "Iter 66560, Minibatch Loss= 0.166720, Training Accuracy= 0.92188\n",
      "Iter 67840, Minibatch Loss= 0.131279, Training Accuracy= 0.96875\n",
      "Iter 69120, Minibatch Loss= 0.094474, Training Accuracy= 0.98438\n",
      "Iter 70400, Minibatch Loss= 0.131610, Training Accuracy= 0.94531\n",
      "Iter 71680, Minibatch Loss= 0.083674, Training Accuracy= 0.97656\n",
      "Iter 72960, Minibatch Loss= 0.148641, Training Accuracy= 0.95312\n",
      "Iter 74240, Minibatch Loss= 0.188409, Training Accuracy= 0.95312\n",
      "Iter 75520, Minibatch Loss= 0.054259, Training Accuracy= 0.97656\n",
      "Iter 76800, Minibatch Loss= 0.169739, Training Accuracy= 0.95312\n",
      "Iter 78080, Minibatch Loss= 0.151092, Training Accuracy= 0.92188\n",
      "Iter 79360, Minibatch Loss= 0.074949, Training Accuracy= 0.98438\n",
      "Iter 80640, Minibatch Loss= 0.162627, Training Accuracy= 0.96875\n",
      "Iter 81920, Minibatch Loss= 0.117770, Training Accuracy= 0.95312\n",
      "Iter 83200, Minibatch Loss= 0.057605, Training Accuracy= 0.99219\n",
      "Iter 84480, Minibatch Loss= 0.054808, Training Accuracy= 0.98438\n",
      "Iter 85760, Minibatch Loss= 0.143255, Training Accuracy= 0.94531\n",
      "Iter 87040, Minibatch Loss= 0.044788, Training Accuracy= 0.97656\n",
      "Iter 88320, Minibatch Loss= 0.111623, Training Accuracy= 0.96094\n",
      "Iter 89600, Minibatch Loss= 0.116111, Training Accuracy= 0.96094\n",
      "Iter 90880, Minibatch Loss= 0.116439, Training Accuracy= 0.96094\n",
      "Iter 92160, Minibatch Loss= 0.058390, Training Accuracy= 0.96875\n",
      "Iter 93440, Minibatch Loss= 0.215650, Training Accuracy= 0.95312\n",
      "Iter 94720, Minibatch Loss= 0.062143, Training Accuracy= 0.99219\n",
      "Iter 96000, Minibatch Loss= 0.149846, Training Accuracy= 0.94531\n",
      "Iter 97280, Minibatch Loss= 0.124460, Training Accuracy= 0.94531\n",
      "Iter 98560, Minibatch Loss= 0.082121, Training Accuracy= 0.97656\n",
      "Iter 99840, Minibatch Loss= 0.118249, Training Accuracy= 0.97656\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.984375\n"
     ]
    }
   ],
   "source": [
    "# recurrent neural network model\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 100000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 28 # MNIST data input (img shape: 28*28)\n",
    "n_steps = 28 # timesteps\n",
    "n_hidden = 128 # hidden layer num of features\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, n_steps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
